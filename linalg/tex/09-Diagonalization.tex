\section*{9 Diagonalisation}

\paragraph{T 9.1.1} Let $A\in\mathbb{R}^{\nxn}$ be a matrix with a
complete set of real eigenvectors and let
$v_1,\dots,v_n\in\mathbb{R}^{n}$ be a basis formed with
eigenvectors of $A$ and let $\lambda_1,\dots,\lambda_n$ be the associated
eigenvalues ($\lambda_i$ associated to $v_i$). Let $V$ be the matrix
whose columns are the eigenvectors. Moreover, let $\Lambda$ be a diagonal
matrix with $\Lambda_{ii}=\lambda_{i}$ and $\Lambda_{ij}=0$ for all $i\not=j$.
Then $A=V\Lambda V^{-1}$.

\paragraph{D 9.1.2} $A\in\mathbb{R}^{\nxn}$ is called a \key{diagonalizable matrix} if there exists an inverse matrix $V$ such that $V^{-1}AV=\Lambda$, where $\Lambda$ is a diagonal matrix. $A=V\Lambda V^{-1}$.

\paragraph{D 9.1.3} If, given a matrix $A\in\mathbb{R}^{\nxn}$,
we can build a basis of $\mathbb{R}^n$ with eigenvectors of $A$, we say that
$A$ has a \key{complete set of eigenvectors}.

\paragraph{P 9.1.6} Let $P$ be a projection matrix on the subspace $U\subseteq \mathbb{R}^n$. Then $P$ has two eigenvalues, $0$ and $1$, and a complete set of real eigenvectors.

\paragraph{D 9.1.7} We say that $A\in\mathbb{R}^{\nxn}$ and
$B\in\mathbb{R}^{\nxn}$ are \key{similar matrices} if there exists an
invertible matrix $S$ such that $B=S^{-1}AS$.

\paragraph{P 9.1.8} Similar matrices have the same eigenvalues.

\paragraph{D 9.1.10} Given a matrix $A\in\mathbb{R}^{\nxn}$ and
an eigenvalue $\lambda$ of $A$, we call the dimension of $\nullsp(A-\lambda I)$
the \key{geometric multiplicity} of $\lambda$.

\paragraph{L 9.1.11} A matrix has a complete set of real eigenvectors if all its 
eigenvalues are real and the geometric multiplicities are the same as the
algebraic multiplicities of all its eigenvalues.

\subsection{Spectral Theorem}

\paragraph{T 9.2.1} (\key{Spectral Theorem}) Any symmetric matrix
$A\in\mathbb{R}^{\nxn}$ has $n$ real eigenvalues and an orthonormal basis of $\mathbb{R}^n$ made of eigenvectors of $A$.

\paragraph{C 9.2.2} Let $A\in\mathbb{R}^{\nxn}$ be a symmetric matrix and let $\Lambda\in\mathbb{R}^{\nxn}$ be a diagonal matrix with the eigenvalues of $A$ on its diagonal. Then there exists an orthogonal matrix $V\in\mathbb{R}^{\nxn}$ (whose columns are eigenvectors of $A$) such that $A=V\Lambda V^\top$.

\paragraph{C 9.2.4} The rank of a real symmetric matrix is the
number fo non-zero eigenvalues (counting repetitions).

\paragraph{P 9.2.6} $A \in \mathbb{R}^{\nxn}$, symmetric,
$v_1, \dots, v_n$ orhonormal basis of eigenvectors of $A$ and
$\lambda_1, \dots, \lambda_n$ the associated eigenvalues. Then
$A = \sum_{k=1}^{n}\lambda_i v_i v_i^\top$.

\paragraph{L 9.2.7} $A\in\mathbb{R}^{\nxn}$ symmetric,
$\lambda_1\neq\lambda_2\in\mathbb{R}$ two eigenvalues of
$A$ with corresponding eigenvectors $v_1,v_2$. Then, $v_1$ and $v_2$ are orthogonal.

\paragraph{L 9.2.8} $A\in\mathbb{R}^{\nxn}$ symmetric, $\lambda\in\CC$ eigenvalue of $A$. Then, $\lambda\in\mathbb{R}$.

\paragraph{P 9.2.10} Given a symmetric matrix $A\in\mathbb{R}^{\nxn}$ the \key{Rayleigh quotient}, defined for $x\in\mathbb{R}^n\setminus\{0\}$, as $R(x)=\frac{x^\top Ax}{x^\top x}$ attains its maximum at $R(v_{\max})=\lambda_{\max}$ and its minimum at $R(v_{\min})=\lambda_{\min}$, where $\lambda_{max}$ and $\lambda_{\min}$ are the largest and smallest eigenvalue of $A$, respectively, and $v_{\max}$ and $v_{\min}$ their associated eigenvectors.

\paragraph{D 9.2.11} A symmetric matrix $A\in\mathbb{R}^{\nxn}$
is said to be \key{positive semidefinite} (PSD) if all its eigenvalues are
non-negative. If all its eigenvalues are, on the other hand, strictly positive,
then we say $A$ is \key{positive definite} (PD).

\paragraph{P 9.2.12} A symmetric matrix $A\in\mathbb{R}^{\nxn}$
is PSD iff $x^\top Ax\geq 0$ for all
$x\in\mathbb{R}^n$. Analogously, a symmetric matrix
$A\in\mathbb{R}^{\nxn}$ is PD iff $x^\top Ax>0$ for
all $x\in\mathbb{R}^n\setminus\{0\}$.

\paragraph{D 9.2.13} Given $n$ vectors,
$v_1,\dots,v_n\in\mathbb{R}^m$, we call their \key{Gram matrix} the
$\nxn$ matrix of their inner products: $G_{ij} = v_i^\top v_j$.

\paragraph{R 9.2.14} Given a matrix $A\in\mathbb{R}^{\mxn}$, as an
abuse of notation, we sometimes also call $AA^\top$ (or $A^\top A$) a Gram
matrix of $A$.

\paragraph{P 9.2.15} Given a real matrix $A\in\mathbb{R}^{m\times
n}$, the non-zero eigenvalues of $A^\top A\in\mathbb{R}^ {\nxn}$ are the
same as the ones of $AA^\top\in\mathbb{R}^{m\times m}$. Both matrices are
symmetric and PSD.

\paragraph{P 9.2.16} Every symmetric PSD matrix $M$
is a Gram matrix of an upper triangular matrix $C$. $M=C^\top C$ is known as
the \key{Cholesky decomposition}.

\subsection{Singular Value Decomposition}

\paragraph{D 9.3.1} (\key{SVD - Singular Value Decomposition})
Let $A\in\mathbb{R}^{\mxn}$. There exist orthogonal matrices
$U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{\nxn}$, such
that $A=U\Sigma V^\top$, where $\Sigma\in\mathbb{R}^{\mxn}$ is a
"diagonal" matrix (as in $\Sigma_{ij} = 0$ when $i \neq j$) and the
diagonal elements are non-negative and ordered in descending order.

The columns $u_1,\dots,u_m$ of $U$ are called the \key{left
singular vectors} of $A$ and are orthonormal. The columns
$v_1,\dots,v_n$ of $V$ are called the \key{right singular vectors}
of $A$ and are orthonormal. The diagonal elements of $\Sigma$,
$\sigma_{i}=\Sigma{ii}$ are called the \key{singular values} of $A$ and are
ordered as $\sigma_1\geq\cdots\geq\sigma_{\min\{m,n\}}$.

\paragraph{R 9.3.2} If $A$ has rank $r$ we can write the SVD in a more
compact form: $A=U_r\Sigma_rV_r^\top$, where $U_r\in\mathbb{R}^{m\times r}$
contains the first $r$ left singular vectors,
$\Sigma_r\in\mathbb{R}^{r\times r}$ is a diagonal matrix with the first $r$
singular values and $V_r\in\mathbb{R}^{n\times r}$ contains the
first $r$ right singular vectors.

Let $A\in\mathbb{R}^{\mxn}$ and $A=U\Sigma V^\top$ be its SVD.
Then $AA^\top = U(\Sigma\Sigma^\top)U^\top$. So the left singular vectors
of $A$, the columns of $U$, are the eigenvectors of $AA^\top$ and the
singular values of $A$ are the square-roots of the eigenvalues of $AA^\top$.
If $m>n$, $A$ has $n$ singular values and $AA^\top$ has $m$ eigenvalues,
but the "missing" ones are $0$.

Analogously: $A^\top A=V(\Sigma^\top\Sigma)V^\top$, and so the right singular
values of $A$, the columns of $V$, are the eigenvectors of $A^\top A$ and the
singular values of $A$ are the square-root of the eigenvalues of $A^\top A$.
If $n>m$, $A$ has $m$ singular values and $A^\top A$ has $n$ eigenvalues, but
the "missing" ones are, again, $0$.

\paragraph{T 9.3.3} Every matrix $A\in\mathbb{R}^{\mxn}$ has an SVD
decomposition of the form shown above. In other words, every linear transformation
is diagonal when viewed in the bases of the singular vectors.

\paragraph{P 9.3.4} Let $A \in \mathbb{R}^{\mxn}$ with rank $r$.
Let $\sigma_1, \dots, \sigma_r$ be the non-zero singular values of $A$,
$u_1, \dots, u_r$ the corresponding left singular vectors and $v_1, \dots, v_r$
the corresponding right singular vectors.
Then $A =  \sum_{k=1}^{r}\sigma_k u_k v_k^\top$
