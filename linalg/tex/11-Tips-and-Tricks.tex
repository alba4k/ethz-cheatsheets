\section*{11 Tips and Tricks}

\subsection{Formulas for \texorpdfstring{$2\times 2$}{2x2} Matrices}

\subsubsection{Determinant}
\begin{equation*}
    \begin{vmatrix}
        a & b \\ c & d
    \end{vmatrix}
    :=
    \det\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix}
    =ad-bc
\end{equation*}

\subsubsection{Inverse}
\begin{equation*}
    A=
    \begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix}
    \quad \dot\Longrightarrow \quad
    A^{-1}=
    \frac{1}{\det(A)}
    \begin{bmatrix}
        d & -b \\ -c & a
    \end{bmatrix}
\end{equation*}

\subsection{Roots of polynomials of degree \texorpdfstring{$2$}{2}}

Let $f(x)$ be a polynomial in $x$: $f(x)=ax^2+bx+c$.

Then the two roots $r_1,r_2$ of $f(x)$ can be computed as follows:

\begin{equation*}
    r_{1,2}=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
\end{equation*}

\subsection{Singular Value Decomposition}
To get the SVD of a matrix $A \in \mathbb{R}^{\mxn}$, we first
find the matrices $S_L = A A^\top \in \mathbb{R}^{m \times m}$ and
$S_R = A^\top A \in \mathbb{R}^{\nxn}$.

Both matrices are symmetric, meaning that their eigenvectors are orthogonal
to eachother. Let $\lambda_1, \dots, \lambda_m$ be the eigenvalues of $S_L$
and $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $S_R$, both ordered
decreasingly. $\lambda_1, \dots, \lambda_{\min(m, n)}$ (i.e. the $\min(m, n)$)
greatest eigenvalues are the same for both matrices, the remaining ones are 0.
Both matrices are PSD and their eigenvalues are therefore non-negative.

For $i \in [\min(m, n)]$, we define $\sigma_i = \sqrt{\lambda_i}$. This is the
$i$-th largest singular value of $A$. 

We define $\Sigma \in \mathbb{R}^{\mxn}$ as the rectangular diagonal matrix
where $\Sigma_{ii} = \sigma_i$.

We define $U \in \mathbb{R}^{m \times m}$ as the orthogonal matrix where the $i$-th
column is the normalized eigenvector of $S_L$ corresponding to $\lambda_i$ (these are the
left singular vectors).

We define $V \in \mathbb{R}^{\nxn}$ as the orthogonal matrix where the $i$-th
column is the normalized eigenvector of $S_R$ corresponding to $\lambda_i$ (there are the
right singular vectors).

Now, these three matrices can be combined into the final SVD: $$A = U \Sigma V^\top$$

\subsection{SVD of \texorpdfstring{$vw^\top$}{vw\^T}}
Let $n \in \NN^+$, Consider arbitrary non-zero vectors
$v, w \in \mathbb{R}^n$. $A \in \mathbb{R}^{\nxn}$ (which is a matrix of rank at most 1) has SVD:
$$A = U\Sigma V^\top = \frac{v}{||v||}(||v|| \cdot ||w||)\left(\frac{w}{||w||}\right)^\top$$

\subsection{Some Useful Equivalences}
\begin{enumerate}
    \item Invertibility ($A \in \mathbb{R}^{\nxn}$):
        \begin{itemize}
            \item $A$ is invertible ($A^{-1}$ exists)
            \item $\det(A) \neq 0$ ($A$ is non-singular)
            \item $0$ is \textbf{not} an eigenvalue ($\forall i \, \lambda_i \neq 0$)
            \item $\rank(A) = n$ (full rank)
            \item The columns of $A$ are linearly independent
            \item $Ax = 0$ has only the trivial soltion ($N(A) = \{0\}$)
            \item $Ax = b$ has a unique solution for every $b \in \mathbb{R}^n$
            \item The RREF of $A$ is $I$
            \item All singular values are non-zero ($\forall i \, \sigma_i > 0$)
        \end{itemize}
    \item Linear independence ($v_1, \dots, v_k \in \mathbb{R}^n$):
        \begin{itemize}
            \item Vectors $v_1, \dots, v_k$ are linearly independent
            \item $\sum_{i=1}^{k}c_i v_i = 0 \implies \forall i \, c_i = 0$
            \item None of the vectors is a linear combination of the others (nor of the preceeding ones)
            \item The matrix $A = \begin{bmatrix}v_1 & \dots & v_k\end{bmatrix}$ has full column rank ($N(A) = \{0\}$)
        \end{itemize}
    \item Diagonalizability ($A \in \mathbb{R}^{\nxn}$):
        \begin{itemize}
            \item $A$ is diagonalizable ($A = V \Lambda V^{-1}$)
            \item $A$ has $n$ linearly independent eigenvectors
            \item The eigenvectors of $A$ form a basis for $\mathbb{R}^n$
            \item For every eigenvalue $\lambda$: Geom. mult. = Alg. mult
            \item The sum of all Geom. mult. is $n$
        \end{itemize}
        \textit{Note}: $n$ distinct eivenvalues $\implies$ diagonalizable \\
        \textit{Note}: $A$ symmetric $\implies$ orthogonally diagonalizeable
    \item Orthogonal matrix ($Q \in \mathbb{R}^{\nxn}$):
        \begin{itemize}
            \item $Q$ is orthogonal
            \item $Q^\top Q = I$
            \item $Q^{-1} = Q^\top$
            \item The columns of $Q$ form an orthonormal basis of $\mathbb{R}^n$
            \item The rows of $Q$ form an orthonormal basis of $\mathbb{R}^n$
            \item $||Qx|| = ||x||$ for all $x \in \mathbb{R}^n$ and $(Qx) \cdot
                  (Qy) = x \cdot y$ for all $x, y \in \mathbb{R}^{\nxn}$
        \end{itemize}
    \item Positive (semi-)definite matrix ($A = A^\top \in \RR^{\nxn}$)
        \begin{itemize}
            \item $\lambda_i > 0$ ($\geq$) for $i = 1, \dots, n$
            \item $x^\top Ax > 0$ ($\geq$) for all $x \neq 0$
            \item All pivots (from gaussian elimination without swaps) are $> 0$
            \item All upper-left sub-determinants are $> 0$ (Sylvester's Criterion)
        \end{itemize}
\end{enumerate}

\subsection{Quick Projections}
\begin{enumerate}
    \item Progection $p$ of $b$ onto $a$:
        $$p = \frac{a^\top b}{a^\top a} a$$
        \textbf{Check}: $e = b-p$ orthogonal to $a$ (i.e., $a^\top e = 0$)
    \item Find $\hat{x}$ that minimizes $||Ax - b||^2$ (least squares approx.):
        \begin{enumerate}
            \item Calculate $M = A^\top A$
            \item Calculate $d = A^\top b$
            \item Solve $M \hat{x} = d$
        \end{enumerate}
        \textbf{Note}: if $A$ has independent columns, $\hat{x} = (A^\top A)^{-1}A^\top b$
    \item Projection of $b$ onto $S \subseteq \mathbb{R}^m$
        \begin{enumerate}
            \item Find basis for $S$, construct matrix $A$
            \item Find $\hat{x}$ using least squares approx.
            \item Find $p = A\hat{x}$
        \end{enumerate}
        \textbf{Note}: Projection matrix: $P = A(A^\top A)^{-1}A^\top$
\end{enumerate}

\subsection{Diagonalization \texorpdfstring{($A = V \Lambda V^{-1} \in \mathbb{R}^{\nxn}$)}{}}
\begin{enumerate}
    \item Find eigenvalues $\lambda_1, \dots, \lambda_n$
    \item Find $n$ independent eigenvectors $v_i$ (if there aren't $n$, $A$ is not diagonalizable)
    \item Construct matrices:
        \begin{itemize}
            \item $\Lambda \in \mathbb{R}^{\nxn}$ has $\lambda_i$ on the diagonal
            \item $V \in \mathbb{R}^{\nxn}$ has $v_i$ as $i$-th column
        \end{itemize}
\end{enumerate}

The \key{Spectral Decomposition} is basically the diagonalization
of a symmetric matrix ($A$ = $A^\top$):
\begin{itemize}
    \item All eigenvalues will be real 
    \item As there are $n$ eigenvalues, there are $n$ independent $v_i$
    \item Eigenvectors are normalized ($||v_i|| = 1$)
    \item We write $A = Q\lambda Q^\top$ (Where $Q$ is orthogonal)
\end{itemize}

\subsection{Quick Sanity-Checks}
\begin{itemize}
    \item \textbf{Trace}: $\tr(A) = \sum a_ii = \sum \lambda_i$ (trace = sum of EW)
    \item \textbf{Determinant}: $\det(A) = \prod \lambda_i$ (det = prod of EW)
    \item \textbf{Rank}: $\rank(A) = \dim(\colsp(A)) = \dim(\rowsp(A)) = $ \# non-zero SW
    \item \textbf{Orthogonality}: $Q^\top Q = I$, $\det(Q) = \pm 1$, $||Qx|| = ||x||$
\end{itemize}

\subsection{Common Proofs}
\begin{itemize}
    \item Prove that $U \subseteq V$ is a subspace:
        \begin{enumerate}
            \item Show $0 \in U$
            \item Let $u, v \in U$, $\lambda \in \RR$. Show $\lambda u + v \in U$.
        \end{enumerate}
        Alternatively, a good counterexample is usually showing
        closure doesn't hold for some vector, or $0 \notin U$
    \item Prove $v_1, \dots, v_k \in \RR^n$ linearly independent:
        \begin{enumerate}
            \item Set up equation $\sum \lambda_i v_i = 0$
            \item Show how this implies $\lambda_1 = \dots = \lambda_k = 0$
        \end{enumerate}
        Alternatively, use the vectors as columns of $A \in \RR^{n \times k}$
        and show $\nullsp(A) = \{0\}$ (or $\rank(A) = k$)
    \item Prove properties of $T : V \to W$:
        \begin{itemize}
            \item \textbf{Injectivity} (1-to-1): $\ker(T) = \{0\}$ ($Ax = 0 \Rightarrow x=0$)
            \item \textbf{Surjectivity} (onto): $Im(T) = W$ ($\rank(A) = \dim(W)$)
            \item \textbf{Bijectivity}: Show both (or just one if $\dim(V) = \dim(W)$)
        \end{itemize}
    \item Prove properties of matrix $A$:
        \begin{itemize}
            \item \textbf{Symmetry}: Show $A^\top = A$ (reminder: $(AB)^\top = B^\top A^\top$)
            \item \textbf{Orthogonality}: Show $Q^\top Q = I$
            \item \textbf{PD or PSD}: Show $x^\top A x > 0$ or $x^\top Ax \geq 0$ for all $x \neq 0$
        \end{itemize}
\end{itemize}

\subsection{Random Matrix Algebra Hacks}
\begin{itemize}
    \item Matrices constructed with the standard basis:
        Let $E_{ij} = e_i e_j^\top$ (Matrix with only $e_{ij} = 1$).
        Then $E_{ij} E_{kl} = (e_i e_j^\top)(e_k e_l^\top) = e_i(e_j^\top e_k)e_l^\top$.
        This means that $E_{ij}E_{kl} = 0$ unless $j = k$
    \item Matrices in the form $M = \begin{bmatrix} A & B \\ 0 & D\end{bmatrix}$:
        \begin{itemize}
            \item $\det(M) = \det(A)\cdot \det(D)$.
            \item $\{\text{EW of M}\} = \{\text{EW of A}\} \cup \{\text{EW of D}\}$
        \end{itemize}
    \item Rank properties
        \begin{itemize}
            \item $\rank(AB) \leq \min(\rank(A), \rank(B))$
            \item $\rank(A+B) \leq \rank(A)+\rank(B)$
            \item $\rank(A) + \rank(B) \leq \rank(AB) + n$ (where $A \in \RR^{\mxn}$, $B \in \RR^{n \times k}$)
            \item $\rank(A) = \dim(\colsp(A)) = \dim(\rowsp(A))$
            \item $\rank(A) + \dim(\nullsp(A)) = n$
        \end{itemize}
\end{itemize}
